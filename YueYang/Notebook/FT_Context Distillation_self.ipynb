{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XFm1BJHGVKfg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers datasets torch\n",
    "!pip install -q transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import kl_div, log_softmax\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Load CoLA dataset\n",
    "from datasets import load_dataset\n",
    "cola_data = load_dataset(\"glue\", \"cola\")\n",
    "\n",
    "# Load HANS dataset using pandas\n",
    "hans_train = pd.read_csv('/Users/yueyang/Downloads/heuristics_train_set.txt', sep='\\t')\n",
    "hans_eval = pd.read_csv('/Users/yueyang/Downloads/heuristics_evaluation_set.txt', sep='\\t')\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "# Map gold_label to numeric labels \n",
    "label_mapping = {\"entailment\": 1, \"non-entailment\": 0}\n",
    "\n",
    "# Preprocess HANS data for PyTorch\n",
    "class HANSPreprocessor(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # Tokenize the sentences\n",
    "        encoding = self.tokenizer(\n",
    "            text=row[\"sentence1\"],\n",
    "            text_pair=row[\"sentence2\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoding[\"attention_mask\"].squeeze()\n",
    "        # Map gold_label to numeric label\n",
    "        label = self.label_mapping[row[\"gold_label\"]]\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label,\n",
    "        }\n",
    "\n",
    "# Create PyTorch Datasets for HANS\n",
    "hans_train_dataset = HANSPreprocessor(hans_train, tokenizer)\n",
    "hans_eval_dataset = HANSPreprocessor(hans_eval, tokenizer)\n",
    "\n",
    "# DataLoader for HANS\n",
    "hans_train_loader = DataLoader(hans_train_dataset, batch_size=32, shuffle=True)\n",
    "hans_eval_loader = DataLoader(hans_eval_dataset, batch_size=32)\n",
    "\n",
    "# Preprocess CoLA dataset for PyTorch\n",
    "# def preprocess_cola(batch):\n",
    "#     encoding = tokenizer(\n",
    "#         batch[\"sentence\"],\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=128,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     return {\n",
    "#         \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "#         \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "#         \"labels\": torch.tensor(batch[\"label\"], dtype=torch.long)\n",
    "#     }\n",
    "def preprocess_cola(batch):\n",
    "    encoding = tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Convert to tensor\n",
    "        \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),  # Convert to tensor\n",
    "        \"labels\": torch.tensor(batch[\"label\"], dtype=torch.long)  # Convert label to tensor\n",
    "    }\n",
    "\n",
    "\n",
    "cola_train_dataset = cola_data[\"train\"].map(preprocess_cola)\n",
    "cola_validation_dataset = cola_data[\"validation\"].map(preprocess_cola)\n",
    "\n",
    "cola_train_loader = DataLoader(cola_train_dataset, batch_size=32, shuffle=True)\n",
    "cola_validation_loader = DataLoader(cola_validation_dataset, batch_size=32)\n",
    "\n",
    "# Define collate function for dataloaders\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch])  # Use \"labels\" key here\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "# def collate_fn(batch):\n",
    "#     input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "#     attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "#     labels = torch.stack([item[\"labels\"] for item in batch])  # Use torch.stack here\n",
    "#     return {\n",
    "#         \"input_ids\": input_ids,\n",
    "#         \"attention_mask\": attention_mask,\n",
    "#         \"labels\": labels\n",
    "#     }\n",
    "\n",
    "# Update DataLoaders with collate_fn\n",
    "cola_train_loader = DataLoader(cola_train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "cola_validation_loader = DataLoader(cola_validation_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "hans_train_loader = DataLoader(hans_train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "hans_eval_loader = DataLoader(hans_eval_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# # Print a sample from HANS DataLoader\n",
    "# print(next(iter(hans_train_loader)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8551/8551 [00:01<00:00, 8041.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'label': 1, 'idx': 0, 'input_ids': [2, 2522, 964, 351, 75, 907, 42, 1966, 6, 905, 1937, 5, 220, 65, 52, 15393, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cola_train_dataset = cola_data[\"train\"].map(preprocess_cola)\n",
    "print(cola_train_dataset[0])  # Verify that input_ids, attention_mask, and labels are tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load teacher (pre-trained model)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-125m\")\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()  # Set teacher to evaluation mode\n",
    "\n",
    "# Load student model (same architecture, will be fine-tuned)\n",
    "student_model = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-125m\")\n",
    "student_model.to(device)\n",
    "\n",
    "# Define the task loss (Cross-Entropy) and distillation loss (KL Divergence)\n",
    "task_loss_fn = CrossEntropyLoss()\n",
    "\n",
    "def distillation_loss_fn(student_logits, teacher_logits):\n",
    "    return kl_div(log_softmax(student_logits, dim=-1), teacher_logits.softmax(dim=-1), reduction=\"batchmean\")\n",
    "\n",
    "# Optimizer for student model\n",
    "optimizer = AdamW(student_model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJa9aLvHVaDE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  28%|██▊       | 76/268 [05:21<13:58,  4.37s/it]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1\n",
    "results_df = pd.DataFrame(columns=[\"epoch\", \"in_domain_accuracy\", \"out_domain_accuracy\"])\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase with CoLA train loader\n",
    "    student_model.train()\n",
    "    for batch in tqdm(cola_train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass (teacher model)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Forward pass (student model)\n",
    "        student_logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute losses\n",
    "        distillation_loss = distillation_loss_fn(student_logits, teacher_logits)\n",
    "        classification_loss = task_loss_fn(student_logits, labels)\n",
    "        loss = 0.5 * distillation_loss + 0.5 * classification_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluation Phase\n",
    "    student_model.eval()\n",
    "    in_domain_correct, in_domain_total = 0, 0\n",
    "    out_domain_correct, out_domain_total = 0, 0\n",
    "\n",
    "    # In-domain (CoLA validation loader)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(cola_validation_loader, desc=\"Evaluating In-Domain\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            in_domain_correct += (predictions == labels).sum().item()\n",
    "            in_domain_total += labels.size(0)\n",
    "\n",
    "    # Out-of-domain (HANS eval loader)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(hans_eval_loader, desc=\"Evaluating Out-Domain\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            logits = student_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            out_domain_correct += (predictions == labels).sum().item()\n",
    "            out_domain_total += labels.size(0)\n",
    "\n",
    "    # Compute accuracies\n",
    "    in_domain_accuracy = in_domain_correct / in_domain_total\n",
    "    out_domain_accuracy = out_domain_correct / out_domain_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: In-Domain Accuracy = {in_domain_accuracy:.4f}, Out-of-Domain Accuracy = {out_domain_accuracy:.4f}\")\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        \"epoch\": [epoch + 1],\n",
    "        \"in_domain_accuracy\": [in_domain_accuracy],\n",
    "        \"out_domain_accuracy\": [out_domain_accuracy]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "    # # Save results to a CSV file after each epoch\n",
    "    # results_df.to_csv(\"ContextDistillation_Cola.csv\", index=False)\n",
    "\n",
    "# Final save of the DataFrame after training\n",
    "results_df.to_csv(\"ContextDistillation_Cola.csv\", index=False)\n",
    "print(\"Training results saved to final_training_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
