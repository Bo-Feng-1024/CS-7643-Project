{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Intro: In-Context Learning Experiment"],"metadata":{"id":"UaJIlfndRx2s"}},{"cell_type":"markdown","source":["reference repo: https://github.com/uds-lsv/llmft/tree/main"],"metadata":{"id":"ohytlhJuR6gU"}},{"cell_type":"markdown","source":["- Experiment: In-Context Learning\n","- Model: opt-125M/350M\n","- Datasets: RTE for in-domain and HANS for out-of-domain\n","- vary on num of shots: 2, 32, 128\n","- prompt format: GPT type\n","```plain\n","{text1} question: {text2} Yes or No?\n","answer: [Yes/No]\n","```"],"metadata":{"id":"1rN6M82vqpcZ"}},{"cell_type":"markdown","source":["what does in-context learning (ICL) here mean:\n","\n","- Instead of updateing the pre-trained model's weight, ICL solve tasks by conditioning on a sequence of demonstrations (i.e. input _x_ and its ground truth _y_ combined by specific pattern).\n","\n","- ICL thus feeds the model a sequence of such demonstrations, followed by the test input (modified by applying the pattern transformation). The language model is then expected to predict the label of this final data point."],"metadata":{"id":"X6YEu5FFVa6p"}},{"cell_type":"markdown","source":["bash code for experiment: `bash $PROJECT_DIR/scripts/in_context/mnli/run_gpt3.sh rte 2 facebook/opt-30b 1 60000`\n","\n","- Task name: \"rte\"\n","- Number of shots: 2\n","- Model: facebook/opt-30b\n","- GPU: 1\n","- Port: 60000"],"metadata":{"id":"mC5-uJbdSB60"}},{"cell_type":"markdown","source":["Other Hyperparameters (all same as the original paper):\n","\n","- fixed context size: 2048 tokens"],"metadata":{"id":"uqIqlC_udpwh"}},{"cell_type":"markdown","source":["Experiment Process:\n","\n","- in-domain: we measure indomain generalization by measuring accuracy on the validation set of each dataset. So in this experiment, the demonstrations are from RTE's training dataset. And the test dataset is RTE's test one.\n","\n","- out-of-domain: we focus on generalization to challenge datasets, designed to test whether models adopt a particular heuristic, or make predictions based on spurious correlations during inference. So in this experiment, the demonstrations are also from RTE's training dataset. And And the test dataset is HANS validation dataset."],"metadata":{"id":"WzS_ZhUcUeqU"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-6oE5hnVfkB","outputId":"f8b35b62-db62-4988-85f3-f0ec79c4c590","executionInfo":{"status":"ok","timestamp":1733937781357,"user_tz":300,"elapsed":28060,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs7643-group-project/notebooks"],"metadata":{"id":"vfn0qJLsg4wx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e73439eb-6924-4431-a978-7d4bae453ffd","executionInfo":{"status":"ok","timestamp":1733937781358,"user_tz":300,"elapsed":5,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/cs7643-group-project/notebooks\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weZOPDg7hid7","outputId":"d46ecc70-3175-4eae-eb81-c5929a2691ed","executionInfo":{"status":"ok","timestamp":1733937784758,"user_tz":300,"elapsed":310,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["few_shot_context_distillation_mnli.ipynb        \u001b[0m\u001b[01;34moffload_folder\u001b[0m/\n","few_shot_context_distillation_rte.ipynb         \u001b[01;34mresults\u001b[0m/\n","few_shot_context_distillation_rts.ipynb         vanilla_cola_baseline.ipynb\n","few_shot_ICL_rte_baseline_results_opt-125M.csv  \u001b[01;34mwandb\u001b[0m/\n","ICL_rte.ipynb\n"]}]},{"cell_type":"code","source":["!pip install -q transformers accelerate bitsandbytes datasets"],"metadata":{"id":"bAhrMw3EmElS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733937793761,"user_tz":300,"elapsed":6963,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}},"outputId":"189b3fcf-b060-4804-de76-03557456097b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["# Dependency and Config"],"metadata":{"id":"wuf84ZmXW4HB"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n","from datasets import load_dataset, ClassLabel\n","import logging\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import numpy as np\n","\n","import time\n","import pandas as pd"],"metadata":{"id":"0b4AQE4gW_gk","executionInfo":{"status":"ok","timestamp":1733940556227,"user_tz":300,"elapsed":85,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","\n","# for reproducibility\n","np.random.seed(42)\n","\n","torch.manual_seed(42)\n","\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed_all(42)"],"metadata":{"id":"s_MENoekXDrY","executionInfo":{"status":"ok","timestamp":1733937799776,"user_tz":300,"elapsed":9,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6M2FfpdXZAq","executionInfo":{"status":"ok","timestamp":1733937799776,"user_tz":300,"elapsed":7,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}},"outputId":"2e023e0a-7a73-472f-f6bb-ead9480574ea"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["# Prep"],"metadata":{"id":"OfYLlcYlXOHl"}},{"cell_type":"code","source":["# originally in task_utils.py\n","task_to_keys = {\n","    # labels are: 0 (entailment), 1 (contradiction)\n","    \"rte\": (\"sentence1\", \"sentence2\"),\n","    \"mnli\": (\"premise\", \"hypothesis\"),\n","    \"mnli-original\": (\"premise\", \"hypothesis\"),\n","    \"mnli-mismatched\": (\"premise\", \"hypothesis\"),\n","    \"hans\": (\"premise\", \"hypothesis\"),\n","\n","    # labels are: 0 (not_duplicate), 1 (duplicate)\n","    \"qqp\": (\"question1\", \"question2\"),\n","    \"paws-qqp\": (\"sentence1\", \"sentence2\"),\n","\n","    # labels are: 0 (not acceptable), 1 (acceptable)\n","    \"cola\": (\"sentence\", None),\n","    \"cola-ood\": (\"sentence\", None),\n","}"],"metadata":{"id":"vgOFQMDskmmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7hR3QZDH7o6_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Loading"],"metadata":{"id":"0Lv23y1Zw8cW"}},{"cell_type":"code","source":["def load_glue_datasets(task_name):\n","    \"\"\"\n","    Get the datasets: specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n","\n","    In distributed training, the load_dataset function guarantee that only one local process can concurrently download the dataset.\n","\n","    originally in task_utils.py\n","    \"\"\"\n","    if task_name is not None:\n","        if task_name == \"mnli\":\n","            # convert to binary format (remove neutral class)\n","            raw_datasets = load_dataset(\n","                \"glue\", task_name)\n","\n","            raw_datasets = raw_datasets.filter(\n","                lambda example: example[\"label\"] != 1)\n","\n","            # change labels of contradiction examples from 2 to 1\n","            def change_label(example):\n","                example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n","                return example\n","            raw_datasets = raw_datasets.map(change_label)\n","\n","            # change features to reflect the new labels\n","            features = raw_datasets[\"train\"].features.copy()\n","            features[\"label\"] = ClassLabel(\n","                num_classes=2, names=['entailment', 'contradiction'], id=None)\n","            raw_datasets = raw_datasets.cast(\n","                features)  # overwrite old features\n","\n","        elif task_name == \"mnli-original\":\n","            # convert to binary format (merge neutral and contradiction class)\n","            raw_datasets = load_dataset(\n","                path=\"glue\", name=\"mnli\")\n","\n","            # change labels of contradiction examples from 2 to 1\n","            def change_label(example):\n","                example[\"label\"] = 1 if example[\"label\"] == 2 else example[\"label\"]\n","                return example\n","            raw_datasets = raw_datasets.map(change_label)\n","\n","            # change features to reflect the new labels\n","            features = raw_datasets[\"train\"].features.copy()\n","            features[\"label\"] = ClassLabel(\n","                num_classes=2, names=['entailment', 'contradiction'], id=None)\n","            raw_datasets = raw_datasets.cast(\n","                features)  # overwrite old features\n","\n","        else:\n","            # Downloading and loading a dataset from the hub.\n","            raw_datasets = load_dataset(\n","                \"glue\",\n","                task_name\n","            )\n","\n","            if task_name == \"qqp\":\n","                # we subsample qqp already here because its really big\n","                # make sure we fix the seed here\n","                for split in raw_datasets.keys():\n","                    raw_datasets[split] = raw_datasets[split].select(np.random.choice(\n","                        np.arange(len(raw_datasets[split])), size=1000, replace=False\n","                    ))\n","\n","    # Determine number of labels\n","    is_regression = task_name == \"stsb\"\n","    if not is_regression:\n","        label_list = raw_datasets[\"train\"].features[\"label\"].names\n","        num_labels = len(label_list)\n","    else:\n","        num_labels = 1\n","\n","    return raw_datasets, label_list, num_labels, is_regression"],"metadata":{"id":"fwdPVs-lOG3u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_hans_dataset(heuristic=None, subcase=None, label=None):\n","    \"\"\"\n","    heuristic = {lexical_overlap, subsequence, constituent}\n","    subcase = see HANS_SUBCASES\n","    label = {0 (entailment), 1 (contradiction)}\n","\n","    originally in task_utils.py\n","    \"\"\"\n","    subset = \"hans\"\n","    dataset = load_dataset(\n","        \"hans\", split=\"validation\")\n","\n","    # hans comes without indices, so we add them\n","    indices = list(range(len(dataset)))\n","    dataset = dataset.add_column(name=\"idx\", column=indices)\n","\n","    if heuristic is not None:  # filter dataset based on heuristic\n","        dataset = dataset.filter(\n","            lambda example: example[\"heuristic\"] == heuristic)\n","        subset = f\"{subset}-{heuristic}\"\n","\n","    if subcase is not None:  # filter dataset based on subcase\n","        dataset = dataset.filter(\n","            lambda example: example[\"subcase\"] == subcase)\n","        subset = f\"{subset}-{subcase}\"\n","\n","    if label is not None:  # filter dataset based on label\n","        dataset = dataset.filter(\n","            lambda example: example[\"label\"] == label)\n","        subset = f\"{subset}-{'entailment' if label == 0 else 'contradiction'}\"\n","\n","    return dataset, subset"],"metadata":{"id":"wSjJ32DBnlqt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## In-Context Leearnig Data Preprocess"],"metadata":{"id":"pX3qvIRNxqQP"}},{"cell_type":"code","source":["def _select_subset_by_ids(dataset, indices):\n","  # originally in eval_utils.py\n","    subset = dataset.select(indices)\n","    return subset"],"metadata":{"id":"1g7iX_sRkWse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_balanced_subsets(dataset):\n","  # originally in eval_utils.py\n","    subset_per_label = {}\n","    for label_idx, _ in enumerate(dataset.features[\"label\"].names):\n","        subset_per_label[label_idx] = dataset.filter(\n","            lambda s: s[\"label\"] == label_idx)\n","    return subset_per_label"],"metadata":{"id":"ddM_IP4NgJ1W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _select_random_subset(dataset, num_shots, balanced: bool, seed: int):\n","  # originally in eval_utils.py\n","    # fix seed\n","    np.random.seed(seed)\n","\n","    if num_shots < 1:\n","        return [], []\n","\n","    if balanced:\n","        assert num_shots % 2 == 0, \"a balanced context requires at least one demonstartion per label\"\n","        # select the same number of samples from every label\n","        indices = []  # we collect all indices here\n","        subset_per_label = get_balanced_subsets(dataset)\n","\n","        for _, samples in subset_per_label.items():\n","            subset_indices = samples[\"idx\"]\n","            # select num_shots // 2 samples\n","            subset_indices = np.random.choice(\n","                subset_indices, size=num_shots // 2, replace=False)\n","            indices += list(subset_indices)\n","        assert len(indices) == num_shots\n","    else:\n","        # just select a random subset of samples\n","        indices = np.random.choice(\n","            range(len(dataset)), size=num_shots, replace=False)\n","\n","    # return _select_subset_by_ids(dataset, indices), indices\n","    return _select_subset_by_idx(dataset, indices), indices"],"metadata":{"id":"beoKymzZiyOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _select_subset_by_idx(dataset, indices):\n","  # originally in eval_utils.py\n","    dataset = dataset.filter(\n","        lambda s: s[\"idx\"] in indices)\n","    return dataset"],"metadata":{"id":"CJ6hmn3Uj2mG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_few_shot_context(\n","    dataset_name,\n","    dataset,\n","    num_shots,\n","    pattern,\n","    label_to_tokens,\n","    separate_shots_by=\" \",\n","    description=\"\",\n","    target_prefix=\"\",\n","    from_indices=None,\n","    balanced=False,\n","    shuffle=False,\n","    seed=123\n","):\n","  # originally in eval_utils.py\n","    assert pattern is not None\n","    assert label_to_tokens is not None\n","\n","    # select samples from which the context will be constructed\n","    if from_indices is not None:\n","        demonstrations, indices = _select_subset_by_ids(dataset, from_indices)\n","    else:\n","        demonstrations, indices = _select_random_subset(\n","            dataset, num_shots, balanced, seed)\n","\n","    if shuffle:\n","        if len(demonstrations) > 0:\n","            demonstrations = demonstrations.shuffle(seed)\n","\n","    # create context\n","    context = \"\" if description == \"\" else f\"{description}{separate_shots_by}\"\n","\n","    for sample in demonstrations:\n","        formated_sample = pattern.format(\n","            text1=sample[task_to_keys[dataset_name][0]],\n","            text2=sample[task_to_keys[dataset_name][1]\n","                         ] if task_to_keys[dataset_name][1] is not None else None\n","        )\n","        verbalized_label = label_to_tokens[sample[\"label\"]]\n","        if verbalized_label.startswith(\"Ġ\"):\n","            # we need to remove the leading whitespace from the target token in the context\n","            verbalized_label = verbalized_label[1:]\n","\n","        elif verbalized_label.startswith(\"▁\"):\n","            # we need to remove the leading whitespace from the target token in the context\n","            verbalized_label = verbalized_label[1:]\n","\n","        context += f\"{formated_sample}{target_prefix}{verbalized_label}{separate_shots_by}\"\n","\n","    return context, indices"],"metadata":{"id":"EdHVOkxyhcIs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def add_context_to_dataset(dataset_name, dataset, pattern, context):\n","    def _add_context(samples):\n","        result = {}\n","        modified_inputs = []\n","        key1, key2 = task_to_keys[dataset_name]\n","\n","        for idx in range(len(samples[key1])):\n","            modified_input = f\"{context}{pattern.format(text1=samples[key1][idx], text2=samples[key2][idx])}\"\n","            modified_inputs.append(modified_input)\n","\n","        result[\"modified_input\"] = modified_inputs\n","\n","        return result\n","\n","    dataset = dataset.map(_add_context, batched=True, batch_size=42)\n","\n","    return dataset\n"],"metadata":{"id":"Us-Bc2MyE0mu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples, tokenizer, pattern, target_tokens, dataset_name, max_length, target_prefix):\n","    \"\"\"\n","    Formats inputs in GPT-3 style using a specific pattern\n","    Tokenizes the formatted inputs\n","    Adds labels for evaluation\n","\n","    originally in eval.py\n","    \"\"\"\n","\n","    # Get the appropriate keys based on the task\n","    if dataset_name == \"rte\":\n","      text1 = examples[\"sentence1\"]\n","      text2 = examples[\"sentence2\"]\n","    elif dataset_name == \"hans\":\n","      text1 = examples[\"premise\"]\n","      text2 = examples[\"hypothesis\"]\n","    else:\n","      raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n","\n","    # Set GPT pattern\n","    id_to_target_token = {idx: t for idx, t in enumerate(target_tokens)}\n","\n","    # Format examples\n","    pattern_examples  = [\n","        pattern.format(\n","            text1=text1[i],\n","            text2=text2[i]\n","        )\n","        for i in range(len(text1))\n","    ]\n","    args = (pattern_examples,)\n","    result = tokenizer(*args, padding=\"max_length\",\n","                    max_length=max_length, truncation=True)\n","\n","    # Get tokens\n","    result[\"input_tokens\"] = [tokenizer.convert_ids_to_tokens(\n","        ids) for ids in result[\"input_ids\"]]\n","\n","    # Decode input\n","    result[\"input_text\"] = [tokenizer.decode(\n","        ids) for ids in result[\"input_ids\"]]\n","\n","    # Replace labels by target tokens indices when using lm_head\n","    target_tokens_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n","    result[\"label\"] = [target_tokens_ids[l] for l in examples[\"label\"]]\n","    result[\"label_text\"] = [id_to_target_token[l] if l != -1 else \"unlabeled\"\n","                            for l in examples[\"label\"]]\n","\n","    return result"],"metadata":{"id":"g46Hj2PaOOl8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Loading"],"metadata":{"id":"iRDoppUryLmw"}},{"cell_type":"code","source":["def _load_model(model_name_config: str):\n","    config = AutoConfig.from_pretrained(\n","        f\"facebook/{model_name_config}\",\n","        num_labels=2,\n","        hidden_dropout_prob=0.1,\n","        attention_probs_dropout_prob=0.1\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        f\"facebook/{model_name_config}\"\n","    )\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        f\"facebook/{model_name_config}\",\n","        config=config,\n","    )\n","\n","    return config, tokenizer, model"],"metadata":{"id":"bDVRXHUTN0ce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Result Processing"],"metadata":{"id":"CEN3POMr0gP9"}},{"cell_type":"code","source":["def evaluate_model(model, dataset, tokenizer, task_name, num_shots, pattern, max_length, target_prefix):\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","\n","    # Create DataLoader\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        num_shots=num_shots,\n","        shuffle=False\n","    )\n","\n","    with torch.no_grad():\n","        for num_shots in dataloader:\n","            inputs = preprocess_function(num_shots, tokenizer, task_name, pattern, max_length, target_prefix)\n","            outputs = model(**inputs)\n","            predictions = outputs.logits.argmax(dim=-1)\n","\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(inputs[\"labels\"].cpu().numpy())\n","\n","    # Calculate accuracy\n","    accuracy = sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)\n","    return accuracy"],"metadata":{"id":"R5tIgkITOSfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In both eval.py and ft.py, but with slightly different implementations\n","def compute_metrics(p: EvalPrediction, task_name):\n","    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n","    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n","\n","    if task_name is not None:\n","        result = metric.compute(predictions=preds, references=p.label_ids)\n","        return result\n","    elif is_regression:\n","        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n","    else:\n","        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}"],"metadata":{"id":"BSLH12dHy--4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _add_args_to_results(args, results):\n","    # Save results in a dataframe\n","    results[\"task_description\"] = args.task_description if args.task_description is not None else \" \"\n","    results[\"pattern\"] = args.pattern\n","    results[\"target_tokens\"] = args.target_tokens\n","    results[\"num_shots\"] = args.num_shots\n","    results[\"separate_shots_by\"] = args.separate_shots_by\n","    results[\"balanced\"] = args.balanced\n","    results[\"shuffle\"] = args.shuffle\n","    results[\"target_prefix\"] = args.target_prefix\n","    results[\"group\"] = args.group\n","\n","    return results"],"metadata":{"id":"Er2gnM3U6DfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _create_df(results):\n","    data = {k: [v] for k, v in results.items()}\n","    df = pd.DataFrame.from_dict(data)\n","    return df\n"],"metadata":{"id":"-Bot1rcH6Xli"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run Experiment"],"metadata":{"id":"JNLlONqZVC0Q"}},{"cell_type":"code","source":["# Configuration\n","seed = 42\n","model_name_config = \"opt-125M\"\n","task_name = \"rte\"\n","eval_task_name = \"hans\"\n","\n","# set prompt pattern\n","pattern = \"{text1} question: {text2} Yes or No?\"\n","target_tokens = \"Ä Yes,Ä No\"\n","target_prefix = \" answer: \"\n","\n","num_shots = [2, 32, 128]  # Number of few-shot examples\n","\n","# Load datasets\n","raw_datasets, label_list, num_labels, is_regression = load_glue_datasets(\"rte\")\n","\n","# Loads HANS dataset as additional evaluation data\n","\n","additional_evaluation_datasets = {}\n","for heuristic in [\"lexical_overlap\"]:\n","    # for heuristic in [\"lexical_overlap\", \"subsequence\", \"constituent\"]:\n","    # Load HANS subsets as additional validation data\n","    for label in [0, 1]:\n","        hans_subset, subset_name = load_hans_dataset(\n","            heuristic=heuristic, subcase=None, label=label)\n","        additional_evaluation_datasets[subset_name] = hans_subset\n","\n","# Model Loading and Configuration:\n","config, tokenizer, model = _load_model(model_name_config)\n","\n","# In-Context Learning Setup:\n","for n in num_shots:\n","  # Create prompt with examples of num_shots\n","  context, context_indices = create_few_shot_context(\n","      dataset_name=\"rte\",\n","      dataset=raw_datasets[\"train\"],\n","      num_shots=n,\n","      pattern=pattern,\n","      target_prefix=target_prefix,\n","      target_tokens=target_tokens,\n","      balanced=True,\n","      shuffle=True\n","  )\n","\n","  # Get evaluation datasets\n","  eval_dataset = raw_datasets[\"validation\"]\n","  eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n","  for name, dataset in additional_evaluation_datasets.items():\n","      dataset = dataset.map(preprocess_function, batched=True)\n","\n","  # Evaluation Process:\n","  total_steps = (len(raw_datasets) // 32) * 40\n","  training_args = TrainingArguments(\n","          output_dir=\"./results\",\n","          overwrite_output_dir=True,\n","          num_train_epochs=40,\n","          per_device_train_batch_size=32,\n","          learning_rate=1e-5,\n","          weight_decay=0.0,\n","          save_steps=10_000,\n","          save_total_limit=2,\n","          warmup_steps=int(0.1 * total_steps),\n","          )\n","\n","  # Initialize trainer\n","  trainer = Trainer(\n","            model = model,\n","            args=training_args,\n","            train_dataset = None,\n","            eval_dataset=None,\n","            compute_metrics = compute_metrics,\n","        )\n","\n","  trainer.eval()\n","\n","  # Run evaluation for each dataset\n","  eval_task_names = [\"rte\"] + list(additional_evaluation_datasets.keys())\n","  eval_datasets = [eval_dataset] + list(additional_evaluation_datasets.values())\n","\n","  all_results = {}\n","  for task_name, dataset in zip(eval_task_names, eval_datasets):\n","      outputs = trainer.predict(\n","          dataset,\n","          metric_key_prefix=task_name\n","      )\n","      metrics = outputs.metrics\n","      all_results.update(metrics)\n","\n","  # Results Processing and Saving:\n","    # Add experiment details to results\n","  all_results = _add_args_to_results(in_context_args, all_results)\n","  all_results[\"indices\"] = contex_indices\n","  all_results[\"context\"] = context\n","  all_results[\"data_seed\"] = seed\n","\n","  # Save to CSV\n","  df = _create_df(all_results)\n","  output_file = os.path.join(training_args.output_dir, f\"{file_name}.csv\")\n","  df.to_csv(output_file)"],"metadata":{"id":"FQ2rCY9Hbnjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["    dataset_name,\n","    dataset,\n","    num_shots,\n","    pattern,\n","    label_to_tokens,\n","    separate_shots_by=\" \",\n","    description=\"\",\n","    target_prefix=\"\",\n","    from_indices=None,\n","    balanced=False,\n","    shuffle=False,\n","    seed=123"],"metadata":{"id":"uywyLm7b_QJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results"],"metadata":{"id":"Yo4a8_1FbnrW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733788367095,"user_tz":300,"elapsed":135,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}},"outputId":"160d5819-8193-4a89-a1b0-17660ced6e92"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{2: {'rte_accuracy': 0.4729241877256318, 'hans_accuracy': 0.5},\n"," 32: {'rte_accuracy': 0.4729241877256318, 'hans_accuracy': 0.5},\n"," 128: {'rte_accuracy': 0.4729241877256318, 'hans_accuracy': 0.5}}"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["pd.DataFrame(results).to_csv(f\"./few_shot_ICL_rte_baseline_results_{model_name_config}.csv\")"],"metadata":{"id":"WcjBXs2cbno-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a5IDQ8X11veg"},"execution_count":null,"outputs":[]}]}