{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-6oE5hnVfkB","outputId":"df474b8d-befe-472e-d132-98a13e7db07b","executionInfo":{"status":"ok","timestamp":1733763360698,"user_tz":300,"elapsed":25896,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/cs7643-group-project/notebooks"],"metadata":{"id":"vfn0qJLsg4wx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fa31d608-86d0-4cf8-e7b7-d3e3ca5245aa","executionInfo":{"status":"ok","timestamp":1733783622337,"user_tz":300,"elapsed":139,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/cs7643-group-project/notebooks\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"weZOPDg7hid7","outputId":"f098c091-8de1-48ef-8937-8fde6aea3ad1","executionInfo":{"status":"ok","timestamp":1733783624395,"user_tz":300,"elapsed":128,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["few_shot_context_distillation_mnli.ipynb  ICL_rte.ipynb    vanilla_cola_baseline.ipynb\n","few_shot_context_distillation_rte.ipynb   \u001b[0m\u001b[01;34moffload_folder\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n","few_shot_context_distillation_rts.ipynb   \u001b[01;34mresults\u001b[0m/\n"]}]},{"cell_type":"code","source":["!pip install -q transformers accelerate bitsandbytes datasets"],"metadata":{"id":"bAhrMw3EmElS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dependency and Config"],"metadata":{"id":"wuf84ZmXW4HB"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n","from datasets import load_dataset\n","import logging\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import numpy as np\n","\n","import time\n","import pandas as pd"],"metadata":{"id":"0b4AQE4gW_gk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","\n","# for reproducibility\n","np.random.seed(42)\n","\n","torch.manual_seed(42)\n","\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed_all(42)"],"metadata":{"id":"s_MENoekXDrY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name_config = \"opt-125M\""],"metadata":{"id":"jagfTmzRg9vg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_6M2FfpdXZAq","executionInfo":{"status":"ok","timestamp":1733782534883,"user_tz":300,"elapsed":14,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}},"outputId":"2978d972-6ba9-4dd9-9b66-2bf9642ab58d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["# Data and In-Context Prep"],"metadata":{"id":"OfYLlcYlXOHl"}},{"cell_type":"code","source":["task_to_keys = {\n","    # labels are: 0 (entailment), 1 (contradiction)\n","    \"rte\": (\"sentence1\", \"sentence2\"),\n","    \"mnli\": (\"premise\", \"hypothesis\"),\n","    \"mnli-original\": (\"premise\", \"hypothesis\"),\n","    \"mnli-mismatched\": (\"premise\", \"hypothesis\"),\n","    \"hans\": (\"premise\", \"hypothesis\"),\n","\n","    # labels are: 0 (not_duplicate), 1 (duplicate)\n","    \"qqp\": (\"question1\", \"question2\"),\n","    \"paws-qqp\": (\"sentence1\", \"sentence2\"),\n","\n","    # labels are: 0 (not acceptable), 1 (acceptable)\n","    \"cola\": (\"sentence\", None),\n","    \"cola-ood\": (\"sentence\", None),\n","}"],"metadata":{"id":"vgOFQMDskmmO","executionInfo":{"status":"ok","timestamp":1733783697962,"user_tz":300,"elapsed":131,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["def setup_logging():\n","\n","    \"\"\"\n","    Setup and Logging Function:\n","    This function sets up logging configuration to track the evaluation process.\n","    \"\"\"\n","\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO\n","    )\n","    logger = logging.getLogger(__name__)\n","    return logger"],"metadata":{"id":"wWMy9CwMNv2I","executionInfo":{"status":"ok","timestamp":1733783698701,"user_tz":300,"elapsed":5,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["def load_model_and_tokenizer(model_path = model_name_config):\n","\n","    \"\"\"\n","    Model Loading Function:\n","    - Loads model configuration, tokenizer, and the OPT model\n","    - Sets up FP16 precision for efficiency\n","    - Configures padding tokens\n","    \"\"\"\n","\n","    # Load tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name_config}\")\n","    # Load model\n","    config = AutoConfig.from_pretrained(\n","        f\"facebook/{model_name_config}\",\n","        num_labels=2,\n","        hidden_dropout_prob=0.1,\n","        attention_probs_dropout_prob=0.1\n","    )\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        f\"facebook/{model_name_config}\",\n","        config=config\n","    )\n","    return model, tokenizer, config"],"metadata":{"id":"bDVRXHUTN0ce","executionInfo":{"status":"ok","timestamp":1733783699492,"user_tz":300,"elapsed":7,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["def load_datasets(task_name=\"rte\", eval_task_name=\"hans\"):\n","\n","    \"\"\"\n","    Dataset Loading Function:\n","    - RTE dataset as in-domain evaluation data\n","    - HANS dataset as out-of-domain evaluation data\n","    \"\"\"\n","\n","    # Load RTE (in-domain) and HANS (out-of-domain) datasets\n","    rte_dataset = load_dataset(\"glue\", task_name)\n","    hans_dataset = load_dataset(eval_task_name)\n","\n","    return rte_dataset, hans_dataset"],"metadata":{"id":"fwdPVs-lOG3u","executionInfo":{"status":"ok","timestamp":1733783699798,"user_tz":300,"elapsed":3,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":82,"outputs":[]},{"cell_type":"code","source":["def _select_subset_by_ids(dataset, indices):\n","    subset = dataset.select(indices)\n","    return subset"],"metadata":{"id":"1g7iX_sRkWse","executionInfo":{"status":"ok","timestamp":1733783700076,"user_tz":300,"elapsed":5,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["def get_balanced_subsets(dataset):\n","    subset_per_label = {}\n","    for label_idx, _ in enumerate(dataset.features[\"label\"].names):\n","        subset_per_label[label_idx] = dataset.filter(\n","            lambda s: s[\"label\"] == label_idx)\n","    return subset_per_label"],"metadata":{"id":"ddM_IP4NgJ1W","executionInfo":{"status":"ok","timestamp":1733783700213,"user_tz":300,"elapsed":5,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":84,"outputs":[]},{"cell_type":"code","source":["def _select_random_subset(dataset, num_shots, balanced=False, seed=42):\n","    # fix seed\n","    np.random.seed(seed)\n","\n","    if num_shots < 1:\n","        return [], []\n","\n","    if balanced:\n","        assert num_shots % 2 == 0, \"a balanced context requires at least one demonstartion per label\"\n","        # select the same number of samples from every label\n","        indices = []  # we collect all indices here\n","        subset_per_label = get_balanced_subsets(dataset)\n","\n","        for _, samples in subset_per_label.items():\n","            subset_indices = samples[\"idx\"]\n","            # select num_shots // 2 samples\n","            subset_indices = np.random.choice(\n","                subset_indices, size=num_shots // 2, replace=False)\n","            indices += list(subset_indices)\n","        assert len(indices) == num_shots\n","    else:\n","        # just select a random subset of samples\n","        indices = np.random.choice(\n","            range(len(dataset)), size=num_shots, replace=False)\n","\n","    # return _select_subset_by_ids(dataset, indices), indices\n","    return _select_subset_by_idx(dataset, indices), indices"],"metadata":{"id":"beoKymzZiyOL","executionInfo":{"status":"ok","timestamp":1733783700696,"user_tz":300,"elapsed":4,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["def _select_subset_by_idx(dataset, indices):\n","    dataset = dataset.filter(\n","        lambda s: s[\"idx\"] in indices)\n","    return dataset"],"metadata":{"id":"CJ6hmn3Uj2mG","executionInfo":{"status":"ok","timestamp":1733783701063,"user_tz":300,"elapsed":5,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["def create_few_shot_context(\n","    dataset_name,\n","    dataset,\n","    num_shots,\n","    pattern,\n","    label_to_tokens,\n","    separate_shots_by=\" \",\n","    description=\"\",\n","    target_prefix=\"\",\n","    from_indices=None,\n","    balanced=False,\n","    shuffle=False,\n","    seed=42\n","):\n","    assert pattern is not None\n","    assert label_to_tokens is not None\n","\n","    # select samples from which the context will be constructed\n","    if from_indices is not None:\n","        demonstrations, indices = _select_subset_by_ids(dataset, from_indices)\n","    else:\n","        demonstrations, indices = _select_random_subset(\n","            dataset, num_shots, balanced, seed)\n","\n","    if shuffle:\n","        if len(demonstrations) > 0:\n","            demonstrations = demonstrations.shuffle(seed)\n","\n","    # create context\n","    context = \"\" if description == \"\" else f\"{description}{separate_shots_by}\"\n","\n","    for sample in demonstrations:\n","        formated_sample = pattern.format(\n","            text1=sample[task_to_keys[dataset_name][0]],\n","            text2=sample[task_to_keys[dataset_name][1]\n","                         ] if task_to_keys[dataset_name][1] is not None else None\n","        )\n","        verbalized_label = label_to_tokens[sample[\"label\"]]\n","        if verbalized_label.startswith(\"Ġ\"):\n","            # we need to remove the leading whitespace from the target token in the context\n","            verbalized_label = verbalized_label[1:]\n","\n","        elif verbalized_label.startswith(\"▁\"):\n","            # we need to remove the leading whitespace from the target token in the context\n","            verbalized_label = verbalized_label[1:]\n","\n","        context += f\"{formated_sample}{target_prefix}{verbalized_label}{separate_shots_by}\"\n","\n","    return context, indices"],"metadata":{"id":"EdHVOkxyhcIs","executionInfo":{"status":"ok","timestamp":1733783701441,"user_tz":300,"elapsed":3,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples, tokenizer, task_name, pattern, max_length, target_prefix):\n","    \"\"\"\n","    Formats inputs in GPT-3 style using a specific pattern\n","    Tokenizes the formatted inputs\n","    Adds labels for evaluation\n","    \"\"\"\n","\n","    # Get the appropriate keys based on the task\n","    if task_name == \"rte\":\n","      text1 = examples[\"sentence1\"]\n","      text2 = examples[\"sentence2\"]\n","    elif task_name in [\"hans\", \"mnli\"]:\n","      text1 = examples[\"premise\"]\n","      text2 = examples[\"hypothesis\"]\n","    else:\n","      raise ValueError(f\"Unsupported task: {task_name}\")\n","\n","    # Format inputs\n","    formatted_inputs = [\n","        pattern.format(\n","            text1=text1[i],\n","            text2=text2[i]\n","        )\n","        for i in range(len(text1))\n","    ]\n","\n","    # Tokenize inputs\n","    tokenized = tokenizer(\n","        formatted_inputs,\n","        padding=\"max_length\",\n","        max_length=max_length,\n","        truncation=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Add labels\n","    if \"label\" in examples:\n","        tokenized[\"labels\"] = torch.tensor(examples[\"label\"])\n","\n","    return tokenized"],"metadata":{"id":"g46Hj2PaOOl8","executionInfo":{"status":"ok","timestamp":1733784423505,"user_tz":300,"elapsed":173,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, dataset, tokenizer, task_name, batch_size, pattern, max_length, target_prefix):\n","    model.eval()\n","    all_predictions = []\n","    all_labels = []\n","\n","    # Create DataLoader\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=False\n","    )\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            inputs = preprocess_function(batch, tokenizer, task_name, pattern, max_length, target_prefix)\n","            outputs = model(**inputs)\n","            predictions = outputs.logits.argmax(dim=-1)\n","\n","            all_predictions.extend(predictions.cpu().numpy())\n","            all_labels.extend(inputs[\"labels\"].cpu().numpy())\n","\n","    # Calculate accuracy\n","    accuracy = sum(p == l for p, l in zip(all_predictions, all_labels)) / len(all_labels)\n","    return accuracy"],"metadata":{"id":"R5tIgkITOSfV","executionInfo":{"status":"ok","timestamp":1733784426870,"user_tz":300,"elapsed":101,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["def main():\n","    logger = setup_logging()\n","\n","    # Configuration\n","    batch_size = 32\n","    pattern = \"{text1} question: {text2} Yes or No?\"\n","    target_prefix = \" answer: \"\n","    target_tokens = [\"Yes\", \"No\"]\n","    id_to_target_token = {idx: t for idx, t in enumerate(target_tokens)}\n","    num_shots = [2, 32, 128]  # Number of few-shot examples\n","\n","    # Load model and tokenizer\n","    logger.info(\"Loading model and tokenizer...\")\n","    model, tokenizer, config = load_model_and_tokenizer()\n","\n","    # Load datasets\n","    logger.info(\"Loading datasets...\")\n","    rte_dataset, hans_dataset = load_datasets()\n","\n","    # Results dictionary\n","    results = {}\n","\n","    # Evaluate for each batch size\n","    for n in num_shots:\n","        logger.info(f\"Evaluating with num of shots {n}\")\n","\n","        # Create few-shot context from RTE training set\n","        context = create_few_shot_context(\n","            dataset_name=\"rte\",\n","            dataset=rte_dataset[\"train\"],\n","            num_shots=n,\n","            pattern=pattern,\n","            label_to_tokens=id_to_target_token,\n","            target_prefix=target_prefix,\n","            balanced=True,\n","            shuffle=True\n","        )\n","\n","        # Evaluate on RTE validation set (in-domain): model, dataset, tokenizer, task_name, batch_size, pattern, max_length, target_prefix\n","        rte_accuracy = evaluate_model(\n","            model=model,\n","            dataset=rte_dataset[\"validation\"],\n","            tokenizer=tokenizer,\n","            task_name=\"rte\",\n","            batch_size=batch_size,\n","            pattern=pattern,\n","            max_length = 128,\n","            target_prefix=target_prefix\n","        )\n","\n","        # Evaluate on HANS (out-of-domain)\n","        hans_accuracy = evaluate_model(\n","            model=model,\n","            dataset=hans_dataset[\"validation\"],\n","            tokenizer=tokenizer,\n","            task_name=\"hans\",\n","            batch_size=batch_size,\n","            pattern=pattern,\n","            max_length = 128,\n","            target_prefix=target_prefix\n","        )\n","\n","        results[n] = {\n","            \"rte_accuracy\": rte_accuracy,\n","            \"hans_accuracy\": hans_accuracy\n","        }\n","\n","        logger.info(f\"num of shots {n} results:\")\n","        logger.info(f\"RTE accuracy: {rte_accuracy:.4f}\")\n","        logger.info(f\"HANS accuracy: {hans_accuracy:.4f}\")\n","\n","    return results"],"metadata":{"id":"FQ2rCY9Hbnjp","executionInfo":{"status":"ok","timestamp":1733784501070,"user_tz":300,"elapsed":112,"user":{"displayName":"Bo Feng","userId":"14786095707414123602"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    # Set device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Set random seeds for reproducibility\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed_all(42)\n","\n","    results = main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YDB8Cz53bnmK","outputId":"4b46a97b-8eb5-41f1-b000-7b2c7375d162"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at facebook/opt-125M and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","<ipython-input-92-c84fa653c8fb>:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tokenized[\"labels\"] = torch.tensor(examples[\"label\"])\n"]}]},{"cell_type":"code","source":["results.to_csv(f\"./few_shot_ICL_rte_baseline_results_{model_name_config}.csv\")"],"metadata":{"id":"WcjBXs2cbno-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yo4a8_1FbnrW"},"execution_count":null,"outputs":[]}]}